{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we‚Äôll explore exactly what happens in the tokenization pipeline.\n",
    "\n",
    "# The first type of tokenizer that comes to mind is word-based.\n",
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)\n",
    "#can be a heavy model, if words are reduced then loss of info can happen\n",
    "\n",
    "\n",
    "# Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "\n",
    "# The vocabulary is much smaller.\n",
    "# There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "\n",
    "\n",
    "# Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "# For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù.\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "tokenizer(\"Using a Transformer network is simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# The tokenization process is done by the tokenize() method of the tokenizer:\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tokens to input IDs\n",
    "# The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding\n",
    "# Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:\n",
    "\n",
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\char software\\Data Science Notebooks\\Hugging Face\\tokenizer.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=0'>1</a>\u001b[0m \u001b[39m# Models expect a batch of inputs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=1'>2</a>\u001b[0m \u001b[39m# In the previous exercise you saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model:\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=3'>4</a>\u001b[0m \u001b[39m# Copied\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForSequenceClassification\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/tokenizer.ipynb#ch0000004?line=7'>8</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Models expect a batch of inputs\n",
    "# In the previous exercise you saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model:\n",
    "\n",
    "# Copied\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oh no! Why did this fail? ‚ÄúWe followed the steps from the pipeline in section 2.\n",
    "\n",
    "# The problem is that we sent a single sequence to the model, whereas ü§ó Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence, but if you look closely, you‚Äôll see that it didn‚Äôt just convert the list of input IDs into a tensor, it added a dimension on top of it:\n",
    "\n",
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let‚Äôs try again and add a new dimension:\n",
    "\n",
    "# Copied\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to work around this, we‚Äôll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:\n",
    "\n",
    "# Copied\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention masks\n",
    "# Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).\n",
    "\n",
    "# Let‚Äôs complete the previous example with an attention mask:\n",
    "\n",
    "# Copied\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer sequences\n",
    "# With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
    "\n",
    "# Use a model with a longer supported sequence length.\n",
    "# Truncate your sequences.\n",
    "# Models have different supported sequence lengths, and some specialize in handling very long sequences. Longformer is one example, and another is LED. If you‚Äôre working on a task that requires very long sequences, we recommend you take a look at those models.\n",
    "\n",
    "# Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n",
    "\n",
    "# Copied\n",
    "sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "# In the last few sections, we‚Äôve been trying our best to do most of the work by hand. We‚Äôve explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.\n",
    "\n",
    "# However, as we saw in section 2, the ü§ó Transformers API can handle all of this for us with a high-level function that we‚Äôll dive into here. When you call your tokenizer directly on the sentence, you get back inputs that are ready to pass through your model:\n",
    "\n",
    "# Copied\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we‚Äôll see in some examples below, this method is very powerful. First, it can tokenize a single sequence:\n",
    "\n",
    "# Copied\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "\n",
    "# It also handles multiple sequences at a time, with no change in the API:\n",
    "\n",
    "# Copied\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can pad according to several objectives:\n",
    "\n",
    "# Copied\n",
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can also truncate sequences:\n",
    "\n",
    "# Copied\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "# If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:\n",
    "\n",
    "# Copied\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "\n",
    "# One token ID was added at the beginning, and one at the end. Let‚Äôs decode the two sequences of IDs above to see what this is about:\n",
    "\n",
    "# Copied\n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping up: From tokenizer to model\n",
    "# Now that we‚Äôve seen all the individual steps the tokenizer object uses when applied on texts, let‚Äôs see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API:\n",
    "\n",
    "# Copied\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic usage completed!\n",
    "# Great job following the course up to here! To recap, in this chapter you:\n",
    "\n",
    "# Learned the basic building blocks of a Transformer model.\n",
    "# Learned what makes up a tokenization pipeline.\n",
    "# Saw how to use a Transformer model in practice.\n",
    "# Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.\n",
    "# Set up a tokenizer and a model together to get from text to predictions.\n",
    "# Learned the limitations of input IDs, and learned about attention masks.\n",
    "# Played around with versatile and configurable tokenizer methods.\n",
    "# From now on, you should be able to freely navigate the ü§ó Transformers docs: the vocabulary will sound familiar, and you‚Äôve already seen the methods that you‚Äôll use the majority of the time."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bdc787100fd307bfeae116c0703f5ab1bc3b04de0700ecfd12a4886e11a6e2d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
