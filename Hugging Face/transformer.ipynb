{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to convert output to PyTorch tensors format, PyTorch is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\char software\\Data Science Notebooks\\Hugging Face\\transformer.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=2'>3</a>\u001b[0m \u001b[39m# import PyTorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=4'>5</a>\u001b[0m raw_inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m'\u001b[39m\u001b[39mve been waiting for a HuggingFace course my whole life.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mI hate this so much!\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=7'>8</a>\u001b[0m ]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=8'>9</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(raw_inputs, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/char%20software/Data%20Science%20Notebooks/Hugging%20Face/transformer.ipynb#ch0000002?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2443\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2438'>2439</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2439'>2440</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2440'>2441</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2441'>2442</a>\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2442'>2443</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2443'>2444</a>\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2444'>2445</a>\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2445'>2446</a>\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2446'>2447</a>\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2447'>2448</a>\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2448'>2449</a>\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2449'>2450</a>\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2450'>2451</a>\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2451'>2452</a>\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2452'>2453</a>\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2453'>2454</a>\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2454'>2455</a>\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2455'>2456</a>\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2456'>2457</a>\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2457'>2458</a>\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2458'>2459</a>\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2459'>2460</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2460'>2461</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2461'>2462</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2462'>2463</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2463'>2464</a>\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2464'>2465</a>\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2480'>2481</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2481'>2482</a>\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2634\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2623'>2624</a>\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2624'>2625</a>\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2625'>2626</a>\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2626'>2627</a>\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2630'>2631</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2631'>2632</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2633'>2634</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2634'>2635</a>\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2635'>2636</a>\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2636'>2637</a>\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2637'>2638</a>\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2638'>2639</a>\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2639'>2640</a>\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2640'>2641</a>\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2641'>2642</a>\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2642'>2643</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2643'>2644</a>\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2644'>2645</a>\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2645'>2646</a>\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2646'>2647</a>\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2647'>2648</a>\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2648'>2649</a>\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2649'>2650</a>\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2650'>2651</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=2651'>2652</a>\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:472\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_fast.py?line=469'>470</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m sanitized_tokens[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_fast.py?line=470'>471</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_fast.py?line=471'>472</a>\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(sanitized_tokens, sanitized_encodings, tensor_type\u001b[39m=\u001b[39;49mreturn_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:212\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=207'>208</a>\u001b[0m     n_sequences \u001b[39m=\u001b[39m encoding[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mn_sequences\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=209'>210</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_sequences \u001b[39m=\u001b[39m n_sequences\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=211'>212</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_to_tensors(tensor_type\u001b[39m=\u001b[39;49mtensor_type, prepend_batch_axis\u001b[39m=\u001b[39;49mprepend_batch_axis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:679\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=676'>677</a>\u001b[0m \u001b[39melif\u001b[39;00m tensor_type \u001b[39m==\u001b[39m TensorType\u001b[39m.\u001b[39mPYTORCH:\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=677'>678</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=678'>679</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnable to convert output to PyTorch tensors format, PyTorch is not installed.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=679'>680</a>\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m    <a href='file:///~/AppData/Local/Programs/Python/Python310/lib/site-packages/transformers/tokenization_utils_base.py?line=681'>682</a>\u001b[0m     as_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to convert output to PyTorch tensors format, PyTorch is not installed."
     ]
    }
   ],
   "source": [
    "# To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the return_tensors argument:\n",
    "from transformers import AutoTokenizer\n",
    "# import PyTorch\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "\n",
    "# Here’s what the results look like as PyTorch tensors:\n",
    "# {\n",
    "#     'input_ids': tensor([\n",
    "#         [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
    "#         [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
    "#     ]), \n",
    "#     'attention_mask': tensor([\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#     ])\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2607287301.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [7]\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip3 install torch torchvision torchaudio\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# We can download our pretrained model the same way we did with our tokenizer. 🤗 Transformers provides an AutoModel class which also has a from_pretrained() method:\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "# In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "# A high-dimensional vector?\n",
    "# The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "# Batch size: The number of sequences processed at a time (2 in our example).\n",
    "# Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "# Hidden size: The vector dimension of each model input\n",
    "\n",
    "# It is said to be “high dimensional” because of the last value. The hidden size can be very large (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
    "\n",
    "# We can see this if we feed the inputs we preprocessed to our model:\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# torch.Size([2, 16, 768])\n",
    "# Note that the outputs of 🤗 Transformers models behave like namedtuples or dictionaries. You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), or even by index if you know exactly where the thing you are looking for is (outputs[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the AutoModel class, but AutoModelForSequenceClassification:\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we look at the shape of our inputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):\n",
    "\n",
    "print(outputs.logits.shape)\n",
    "# torch.Size([2, 2])\n",
    "# Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch (from versions: none)\n",
      "ERROR: No matching distribution found for torch\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)\n",
    "# tensor([[-1.5607,  1.6123],\n",
    "        # [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a SoftMax layer (all 🤗 Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "# tensor([[4.0195e-02, 9.5980e-01],\n",
    "#         [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
    "\n",
    "# To get the labels corresponding to each position, we can inspect the id2label attribute of the model config (more on this in the next section):\n",
    "\n",
    "# model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Transformer\n",
    "# The first thing we’ll need to do to initialize a BERT model is load a configuration object:\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The configuration contains many attributes that are used to build the model:\n",
    "\n",
    "print(config)\n",
    "\n",
    "# BertConfig {\n",
    "#   [...]\n",
    "#   \"hidden_size\": 768,\n",
    "#   \"intermediate_size\": 3072,\n",
    "#   \"max_position_embeddings\": 512,\n",
    "#   \"num_attention_heads\": 12,\n",
    "#   \"num_hidden_layers\": 12,\n",
    "#   [...]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different loading methods\n",
    "# Creating a model from the default configuration initializes it with random values:\n",
    "\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in Chapter 1, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it’s imperative to be able to share and reuse models that have already been trained.\n",
    "# \n",
    "# Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:\n",
    "\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving methods\n",
    "# Saving a model is as easy as loading one — we use the save_pretrained() method, which is analogous to the from_pretrained() method:\n",
    "\n",
    "model.save_pretrained(\"directory_on_my_computer\")\n",
    "\n",
    "# This saves two files to your disk:\n",
    "\n",
    "# ls directory_on_my_computer\n",
    "\n",
    "# config.json pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizers can take care of casting the inputs to the appropriate framework’s tensors, but to help you understand what’s going on, we’ll take a quick look at what must be done before sending the inputs to the model.\n",
    "\n",
    "# Let’s say we have a couple of sequences:\n",
    "\n",
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
    "\n",
    "# The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:\n",
    "\n",
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]\n",
    "# This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices). This “array” is already of rectangular shape, so converting it to a tensor is easy:\n",
    "\n",
    "# Copied\n",
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the tensors as inputs to the model\n",
    "# Making use of the tensors with the model is extremely simple — we just call the model with the inputs:\n",
    "\n",
    "output = model(model_inputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bdc787100fd307bfeae116c0703f5ab1bc3b04de0700ecfd12a4886e11a6e2d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
